{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "AAA\n",
      "(400, 600, 3)\n",
      "WARNING:tensorflow:From C:\\Users\\campe\\Miniconda3\\envs\\myfisrtenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\campe\\Miniconda3\\envs\\myfisrtenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Failed to finish task in epsoide 0\n",
      "finished episode 199\n",
      "Failed to finish task in epsoide 1\n",
      "finished episode 199\n",
      "Failed to finish task in epsoide 2\n",
      "finished episode 199\n",
      "Failed to finish task in epsoide 3\n",
      "finished episode 199\n",
      "Failed to finish task in epsoide 4\n",
      "finished episode 199\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "\n",
    "\n",
    "\n",
    "class MountainCarConvolutionalTraining:\n",
    "\n",
    "    def __init__(self, env):\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        self.stack_depth, self.image_height, self.image_width = self.get_model_input_shape()\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.learning_rate = 0.00025\n",
    "        self.train_network = self.create_network()\n",
    "        self.target_network = self.create_network()\n",
    "\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.05\n",
    "        self.epsilon_min = 0.1\n",
    "\n",
    "        self.frames_memory = deque(maxlen=self.stack_depth)\n",
    "        self.replay_buffer = deque(maxlen=20000)\n",
    "        self.num_pick_from_buffer = 32\n",
    "\n",
    "        self.iteration_num = 201  # max is 200\n",
    "\n",
    "        self.episode_num = 400\n",
    "\n",
    "    def get_model_input_shape(self):\n",
    "        self.env.reset()\n",
    "        initial_image_shape = self.env.render(mode='rgb_array').shape\n",
    "        image_height = initial_image_shape[0]\n",
    "        image_width = initial_image_shape[1]\n",
    "        stack_depth = 2\n",
    "\n",
    "        # dimensions are 2 400 600\n",
    "        return stack_depth, image_height, image_width\n",
    "\n",
    "    def create_network(self):\n",
    "        input_shape = (self.stack_depth, self.image_height, self.image_width)\n",
    "\n",
    "        model = models.Sequential()\n",
    "\n",
    "        model.add(layers.Conv2D(32, (8, 8), strides=4, padding=\"same\", activation='relu', input_shape=input_shape,\n",
    "                                name='conv_1'))\n",
    "        model.add(layers.Conv2D(64, (4, 4), strides=2, padding='same', activation='relu', name='conv_2'))\n",
    "        model.add(layers.Conv2D(64, (3, 3), strides=2, padding='same', activation='relu', name='conv_3'))\n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "\n",
    "        model.add(layers.Dense(512, activation='relu', name='dense_1'))\n",
    "        model.add(layers.Dense(self.num_actions, activation='linear', name='output'))\n",
    "\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "\n",
    "        self.epsilon = max(self.epsilon, self.epsilon_min)\n",
    "\n",
    "        if np.random.rand(1) < self.epsilon:\n",
    "            action = np.random.randint(0, 3)\n",
    "        else:\n",
    "            action = np.argmax(self.train_network.predict(state)[0])\n",
    "\n",
    "        return action\n",
    "\n",
    "    def train_from_buffer(self):\n",
    "\n",
    "        if len(self.replay_buffer) < self.num_pick_from_buffer:\n",
    "            return\n",
    "\n",
    "        samples = random.sample(self.replay_buffer, self.num_pick_from_buffer)\n",
    "\n",
    "        states = []\n",
    "        new_states = []\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            states.append(state)\n",
    "            new_states.append(new_state)\n",
    "\n",
    "        states = np.array(states)\n",
    "        new_states = np.array(new_states)\n",
    "\n",
    "        targets = self.train_network.predict(states)\n",
    "        new_state_targets = self.target_network.predict(new_states)\n",
    "\n",
    "        i = 0\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            target = targets[i]\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                Q_future = max(new_state_targets[i])\n",
    "                target[action] = reward + Q_future * 0.99\n",
    "            i += 1\n",
    "\n",
    "        self.train_network.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "    def original_try(self, current_state, eps):\n",
    "\n",
    "        reward_sum = 0\n",
    "\n",
    "        for i in range(self.iteration_num):\n",
    "\n",
    "            best_action = self.get_best_action(current_state)\n",
    "\n",
    "            new_state_numerical, reward, done, _ = self.env.step(best_action)\n",
    "            new_image = self.env.render(mode='rgb_array')\n",
    "            next_frame = self.process_image(new_image)\n",
    "            next_frame = next_frame.reshape(next_frame.shape[0], next_frame.shape[1])\n",
    "\n",
    "            self.frames_memory.append(\n",
    "                next_frame)  # current_state is a FIFO buffer so just by appending the size  of current_state is kept constant\n",
    "\n",
    "            new_state = np.asarray(self.frames_memory)\n",
    "\n",
    "            # # Adjust reward for task completion\n",
    "            if done:\n",
    "                reward += 10\n",
    "\n",
    "            self.replay_buffer.append([current_state, best_action, reward, new_state, done])\n",
    "\n",
    "            self.train_from_buffer()\n",
    "\n",
    "            reward_sum += reward\n",
    "            current_state = new_state\n",
    "\n",
    "            # print(i)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if i >= 199:\n",
    "            print(\"Failed to finish task in epsoide {}\".format(eps))\n",
    "        else:\n",
    "            print(\"Success in epsoide {}, used {} iterations!\".format(eps, i))\n",
    "            self.train_network.save(\n",
    "                '../models/modelsMountainCar/trainNetworkInEPS{}with{}iterations.h5'.format(eps, i))\n",
    "\n",
    "            # SYNC\n",
    "\n",
    "            self.target_network.set_weights(self.train_network.get_weights())\n",
    "\n",
    "            print(\"now epsilon is {}, the reward is {}\".format(max(self.epsilon_min, self.epsilon), reward_sum))\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "\n",
    "        print(\"finished episode {}\".format(i))\n",
    "\n",
    "    def process_image(self, image):\n",
    "        # Simple processing: RGB to GRAY and resizing keeping a fixed aspect ratio\n",
    "        if len(image.shape) == 3:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        return cv2.resize(image, (self.image_width, self.image_height))\n",
    "\n",
    "    def start(self):\n",
    "\n",
    "        for eps in range(self.episode_num):\n",
    "            self.frames_memory.clear()\n",
    "\n",
    "            self.env.reset()\n",
    "\n",
    "            current_image = self.env.render(mode='rgb_array')\n",
    "            current_frame = self.process_image(current_image)  # the frame is an greyscale image of the current position\n",
    "            current_frame = current_frame.reshape(1, current_frame.shape[0], current_frame.shape[1])\n",
    "            current_state = np.repeat(current_frame, self.stack_depth, axis=0)\n",
    "            self.frames_memory.extend(current_state)\n",
    "\n",
    "            self.original_try(current_state, eps)\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "print(\"AAA\")\n",
    "env.reset()\n",
    "\n",
    "\n",
    "print(env.render(mode='rgb_array').shape)\n",
    "\n",
    "agent = MountainCarConvolutionalTraining(env)\n",
    "agent.start()\n",
    "\n",
    "print(\"Yahoo\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}